# Model Comparison Report: GPT-4o vs Claude Sonnet

## Experiment 1 (Cross-Topic Testing)

### GPT-4o Results
- Total Examples: 100
- Exact Match Accuracy: 0.00%
- Semantic Similarity Accuracy (threshold 0.7): 14.00%
- Average Semantic Similarity: 0.55

### Claude Sonnet Results
- Total Examples: 100
- Exact Match Accuracy: 0.00%
- Semantic Similarity Accuracy (threshold 0.7): 3.00%
- Average Semantic Similarity: 0.49

## Experiment 2 (Topic-Constrained Testing)

### GPT-4o Results
- Total Examples: 8
- Exact Match Accuracy: 0.00%
- Semantic Similarity Accuracy (threshold 0.7): 0.00%
- Average Semantic Similarity: 0.54

### Claude Sonnet Results
- Total Examples: 100
- Exact Match Accuracy: 2.00%
- Semantic Similarity Accuracy (threshold 0.7): 17.00%
- Average Semantic Similarity: 0.51

## Analysis Summary

The comparison reveals several key insights:
1. Cross-Topic Testing shows differences in model performance across diverse mathematical topics
2. Topic-Constrained Testing demonstrates each model's ability to leverage domain-specific context
3. Semantic similarity analysis provides a more nuanced view of model understanding beyond exact matches

